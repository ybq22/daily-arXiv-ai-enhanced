<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Gender Bias in Large Language Models for Healthcare: Assignment Consistency and Clinical Implications](https://arxiv.org/abs/2510.08614)
*Mingxuan Liu,Yuhe Ke,Wentao Zhu,Mayli Mertens,Yilin Ning,Jingchi Liao,Chuan Hong,Daniel Shu Wei Ting,Yifan Peng,Danielle S. Bitterman,Marcus Eng Hock Ong,Nan Liu*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在医疗诊断中对患者性别相关性的判断存在不一致性，特别是在性别相关性和必要性评估方面，这可能影响临床实践中AI的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗领域的应用日益广泛，但其可能存在复制或放大性别偏见的风险，这会影响临床决策的公平性和可靠性。

Method: 使用新英格兰医学杂志挑战赛的案例，为多个开源和专有LLM分配性别（女性、男性或未指定），评估其在诊断和患者性别相关性判断方面的一致性。

Result: 大多数模型在诊断方面相对一致，但在患者性别相关性和必要性判断上，所有模型都表现出显著的不一致性，特别是在相关性判断方面，某些模型甚至显示出系统性的性别差异。

Conclusion: 这种未被充分探索的偏见可能削弱LLM在临床实践中的可靠性，强调需要在与LLM交互时常规检查身份分配一致性，以确保可靠和公平的AI支持临床护理。

Abstract: The integration of large language models (LLMs) into healthcare holds promise
to enhance clinical decision-making, yet their susceptibility to biases remains
a critical concern. Gender has long influenced physician behaviors and patient
outcomes, raising concerns that LLMs assuming human-like roles, such as
clinicians or medical educators, may replicate or amplify gender-related
biases. Using case studies from the New England Journal of Medicine Challenge
(NEJM), we assigned genders (female, male, or unspecified) to multiple
open-source and proprietary LLMs. We evaluated their response consistency
across LLM-gender assignments regarding both LLM-based diagnosis and models'
judgments on the clinical relevance or necessity of patient gender. In our
findings, diagnoses were relatively consistent across LLM genders for most
models. However, for patient gender's relevance and necessity in LLM-based
diagnosis, all models demonstrated substantial inconsistency across LLM
genders, particularly for relevance judgements. Some models even displayed a
systematic female-male disparity in their interpretation of patient gender.
These findings present an underexplored bias that could undermine the
reliability of LLMs in clinical practice, underscoring the need for routine
checks of identity-assignment consistency when interacting with LLMs to ensure
reliable and equitable AI-supported clinical care.

</details>
