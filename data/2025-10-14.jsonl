{"id": "2510.08614", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08614", "abs": "https://arxiv.org/abs/2510.08614", "authors": ["Mingxuan Liu", "Yuhe Ke", "Wentao Zhu", "Mayli Mertens", "Yilin Ning", "Jingchi Liao", "Chuan Hong", "Daniel Shu Wei Ting", "Yifan Peng", "Danielle S. Bitterman", "Marcus Eng Hock Ong", "Nan Liu"], "title": "Gender Bias in Large Language Models for Healthcare: Assignment Consistency and Clinical Implications", "comment": null, "summary": "The integration of large language models (LLMs) into healthcare holds promise\nto enhance clinical decision-making, yet their susceptibility to biases remains\na critical concern. Gender has long influenced physician behaviors and patient\noutcomes, raising concerns that LLMs assuming human-like roles, such as\nclinicians or medical educators, may replicate or amplify gender-related\nbiases. Using case studies from the New England Journal of Medicine Challenge\n(NEJM), we assigned genders (female, male, or unspecified) to multiple\nopen-source and proprietary LLMs. We evaluated their response consistency\nacross LLM-gender assignments regarding both LLM-based diagnosis and models'\njudgments on the clinical relevance or necessity of patient gender. In our\nfindings, diagnoses were relatively consistent across LLM genders for most\nmodels. However, for patient gender's relevance and necessity in LLM-based\ndiagnosis, all models demonstrated substantial inconsistency across LLM\ngenders, particularly for relevance judgements. Some models even displayed a\nsystematic female-male disparity in their interpretation of patient gender.\nThese findings present an underexplored bias that could undermine the\nreliability of LLMs in clinical practice, underscoring the need for routine\nchecks of identity-assignment consistency when interacting with LLMs to ensure\nreliable and equitable AI-supported clinical care."}
